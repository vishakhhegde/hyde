---
layout: post
title: Learnings from "Prediction Machines" by Ajay Agarwal, Joshua Gans and Avi Goldfarb
comments: true
img_excerpt: imgs/pred_machines.jpg
---
#### Date: June 11th, 2018

I am quite interested in the economics of the new industrial revolution. I have the lens of a person trained in the technicalities of machine learning and artificial intelligence (and a namesake minor in economics from my undergrad). So my understanding of the effects of AI on the economy and how it might shape the world moving forward is extremely limited. In the past, I have watched a few YouTube/Netflix documentaries on how the industrial revolution via steam engines, the computer revolution, the internet revolution and most recently the cellphone revolution have changed the world and affected the way people live their lives. They were contemplative perspectives. And contemplation can sometimes lead to overfitting and misunderstanding of cause-effect relationships. So, a forward looking book is a good refresher for me. Here are a few things I learned so far:

- The current Artificial Intelligence revolution doesn't bring intelligence. It brings a critical component of intelligence namely prediction.
- Prediction is critical in many businesses and in people's lives. It is involved consciously or sub-consciously in any decision making process.
- Prediction is simply the process of filling in missing information by using the information we have (data), to generate information we do not have.

#### Cheaper predictions mean more predictions
- 200 hundred years ago, when artificial light was beginning to be used, it was 400 times more expensive than it is today. It used to be used limitedly (perhaps in certain streets and rich households). But as it got cheaper, we see it being used everywhere and things rarely imagined before became commonplace (like large building that natural sunlight cannot penetrate, lit up by artificial light). 
- The computer revolution made arithmetic cheaper. Cheap arithmetic is now being used in applications not traditionally arithmetic in nature, like photography (remember the times of dark rooms and chemicals generating our photographs?).
- Recently predictions were used for traditionally prediction intensive tasks like inventory management and demand forecasting. Off late, they are being used to manage and search our digital photos (until recently, it used to be an arithmetic only domain). And in self driving cars!
- Cheap can change strategy: today Amazon uses the shop-then-ship strategy. If predictions become extremely good (in a hypothetical future), it might be cheaper for Amazon to use the ship-then-shop model: Amazon is so sure about individual customers' future consumption pattern, that it ships them the product first and consumers have the option to return it if they don't need it. This new business model can open up many new industries (like logistics for product returns) and can transform the business.
- When something becomes cheap, its complements become more valuable. For predictions they are things like data, judgement, action and realistic simulators.

#### Prediction: What is happening today?
- With the advent of Machine Learning (deep learning in particular for many applications), the cost of producing the same quality of prediction has dropped significantly. We can see this in Computer Vision, Natural Language Processing and Speech Processing (for example). Seeing a computer vision model correctly classify tumors might seem magical. But they are just better predictions.
- In the 1990s, credit card companies caught abuot 80% of fraudulent transactions. In 2000s it was improved to 90-95% and is at 98 - 99.9% today. The last jump has been a result of machine learning. The jump looks small, but are extremely significant. An improvement from 85% to 90% means that mistakes reduce by a third. A jump from 98% to 99.9% means that mistakes fall by a factor of 20. This improvement means that credit card companies can detect and address fraud before a customer notices anything amiss. This is huge and transformational for both the company and the customer.

#### Intelligence and predictions
- Machine learning is different from statistics. Statictics cares about being right on an average. Machine Learning's goal is operational effictiveness (it can be slightly wrong all the time, yet be very useful in practice). Such an approach drove rapid improvements in the field by giving researchers the freedom to experiment. Machine learning uses many elements of statistics to be effective.
-  Machine learning can handle more complexity: consider the problem of predicting customer churn (when the customer ceases the relationship with the product/service) in the cellphone network industry. The data consists of hour by hour call records and standard variables like bill size and payment punctuality. Decades ago, a lot of human effort was involved in identifying the right kind of variables and their interactions in building a regression model. Regression, though sample efficient (it is not data hungry) limits the kind of interactions the input variables can have in the model.  While regression works well in many scenarios, machine learning has been used increasingly to predict churn. With machine learning (deep learning in particular), variables can combine in unexpected ways. For example, it can model cases like 'people with large weekend lon-distance bills who also pay late and tend to text a lot may be particularly likely to churn'. Such interactions are unthinkable in the classical world of regression. 
- Why do people refer to machine learning as artificial intelligence? The technological threshold for what an artificial intelligence is changes every few years. Today, it refers to algorithms that can perform tasks that people thought were associated with human intelligence, like object recognition and translation. However, these very same systems cannot reason and we still find it hard to interpret predictions from such complex algorithms.

#### Role of data in predictions
For the purposes of predictions, data comes in three flavors: the input data, the training data and the feedback data

- The training data: is used to produce a good mathematical model (like obtaining the exact set of weights in a deep neural network).
- Input data: is fed to the algorithm to produce the prediction.
- Feedback data: is used to improve the model in the future.
- Data is fundamental for prediction (the right kind of data). Consider predicting the probability of stroke: this requires individual personal data. We cannot (yet) do without input data.
- Data acquisition is costly. So it is important to have a strategy for it: how frequently do you need to collect data? At what scale? More can mean costly, but can also mean better predictions. What should be the trade-off?
- If the prediction task is well defined, there are good data science tools to asses the amount of data required, given the expected reliability and the need for accuracy (as a rule of thumb: for each feature and for each value that feature can take, have atleast 5 data points for 'good' predictive capability). 
- Predicting stroke requires high frequency of data collection. So startups like Cardiogram use the Apple Watch for data collection. Contrast this with traditional ways of monitoring heart health (by visiting the doctor onces in several weeks). Collecting this data is costly - patients had to wear a device at all times which intruded with their regular routines. Since it is personal data, more engineering had to be put in place to ensure privacy.
- The economics of data collection: From a pure statistical point of view, data has decreasing returns (in terms of prediction accuracy) to scale. Information from the 10th observation is usually more valuable than information from the 1000th observation. From the economics point of view, this might not be true. Data can have increasing returns (in terms of value) to scale. Consider for example the online search industry. Users keep coming back to google since it is great on long-tail search queries (those rare ones that fail on Bing or Yahoo!). The value of a search engine is driven by its ability to give better results on unusual queries. Being a little better in search can lead to big market differences. i.e. data may have increasing returns in scale in some cases.
- Therefore organizations need to understand the relationship between adding more data, enhancing prediction accuracy, and increasing value creation.

#### On Dividing Labor with prediction machines
Adam Smith's eighteenth century economic philosophy involved allocating labor based on relative strengths. Humans and machines both have their strengths and weaknesses. It is essential to identify these strengths and weaknesses of humans and machines in order to strategise on labor allocation in the 'New economy'.

- Humans are sometimes poor at prediction and are biased. This is clearly portrayed in the Brad Pitt movie Moneyball; . There are also experiments which suggest that humans are naive while using statistics and that they fail to account for base probability rates when making decisions. Humans also fail to account for extremeties that arise from small samples.
- Amos Tversky and researchers from Harvard Medical School presented physicians with two treatments for lung cancer, namely radiation or surgery. The 5 year survival rate recommends surgery, which is riskier than radiation. When told that "the one-month survival rate is 90%", 84% of physicians chose surgery, but the rate fell to 50% when told that "there is a 10% mortality in the first month". While both these statements mean the same thing, the framing of the sentance affected their decision. Such biases would not exist in machines.
- Similar (and equally consequential) biases exist in many decisions today in fields ranging from sports to policy and law.
- Machines are also poor at predictions in certain scenarios.
- Here is a very useful structure for understanding conditions under which machine predictions can fail:
- known-knowns is a scenario where we know that we have the correct and rich data to make good predictions (natural image recognition). Machines excel at these
- known-unknowns are scenarios where we know that we do not have enough data to make predictions (Predicting major earthquakes. We have extremely sparse data for this). Humans are pretty good when the data is sparse.
- unknown-unknowns are scenarios where no there is no past experience. This is what makes life interesting (at least for me). The 2008 economic meltdown was unexpected (to most people). It is extremely hard to predict such rare events. both humans and machines fail with these scenarios.
- unknown-knowns are scenarios where we wrongly think we are making the right predictions but with time realise we aren't (unobserved factors of variations that can change with time). Most machine learning models only model correlation between input data and outcomes. They do not model the underlying causal structure explicitly. Consider this case: depression is a major problem among graduate students in academia. Does academia cause these grad students to be more depressed? Or are people entering academia likely to be depressed because of reasons that have nothing to do with academia? It is hard to decipher this problem, because we do not know the outcome of scenarios if students had not chosen an academic path. This is where people perform controlled randomized trials (A/B testing). This is a dangerous scenario where we think the machine predicts well, but the model might not capture the underlying causal factors, in which case predictions fail when the causal factors change over time.
- Sometimes, a combination of predictions from humans and machines generate the best predictions by complementing each others' mistakes. Consider the example of detection of breast cancer. A really good deep learning algorithm made the correct prediction 92.5% of the time, while humans performed at 96.6%. However, combining the predictions of humans with the deep learning algorithm resulted in an accuracy of 99.5%. The errors fell by 85%. This is because humans and machines are good at different things. The human pathologist was usually right when cancer was detected, while the AI was good at predicting the absence of cancer.
- Therefore understanding the strengths and limits of the human and the machine is extremely important. As machine predictions improve, businesses must adjust the division of labor between humans and machines.
- Machines usually fail when it lacks data or if it is radically outside of what it has seen before, while humans are good under this condition. Hence use "prediction by exception": let the machines predict on routine cases with a human intervening on more uncertain cases under lack of data.
- Machine predictions can scale very well and the cost per predictions reduce as more predictions are made. This is not true for humans. Machines can also learn from many human predictions quickly. For example, in medical image recognition, different radiologists interpret the same image differently. But a machine can learn from all the various interpretations.


#### Role of prediction in decisions
A decision consists of many components, even though it might look like a monolithic thought process on the surface. Some of the components are: input data, training, prediction, judgement, action and the outcome of the action.

- Let's say you go to a doctor with stomach pain and frequent gastro-intestinal problems. You are told to take an endoscopy test and gets more information from you. Using this input and various other inputs from years of training as a doctor, she suggests that there is a high chance that you have stomach ulcers. Suppose that if it is an ulcer, the treatment is a medicine with some non lethal long term side effects and a recommendation of stomach friendly diet. If it is not, the recommendation is just a stomach friendly diet. If the doctor mistakenly treats you with the medicine, then there is a chance of long term side effects. However, if you do have ulcers and she treats you with a stomach friendly diet, there is a high chance of serious complications leading to other dietary disorders. This is where judgement comes into picture: determining the relative pay-offs of various actions (recommendations). Lots of other factors might also go into the decision making process (including your age, current diet, smoking habits etc.).
- Judgement is a key complement of prediction. So as predictions become cheaper, judgement becomes more valuable.
- As predictions become cheap, jobs that rely highly on predictions will become less valuable: example is Uber replacing London black cabs drivers who were required to take 'The Knowledge' test that demanded intricate knowledge of all the roads in London. Digital and adaptive maps which recommend best routes to take, opened up opportunities for ordinary people with the Uber app on their phone, to compete with the black cabs drivers.
- Simulators are key components in many industries. Human predictions are fed into simulators to peek into possible outcomes. Large decisions are made based on results from accurate simulators. Simulators thus provide a great framework for judgement. As predictions become cheaper, the value of simulators will increase.