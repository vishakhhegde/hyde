---
layout: post
title: Learnings from "Prediction Machines" by Ajay Agarwal, Joshua Gans and Avi Goldfarb
comments: true
img_excerpt: imgs/pred_machines.jpg
---
#### Date: June 11th, 2018

I am quite interested in the economics of the new industrial revolution. I have the lens of a person trained in the technicalities of machine learning and artificial intelligence (and a namesake minor in economics from my undergrad days). So my understanding of the effects of AI on the economy and how it might shape the world moving forward is extremely limited. In the past, I have watched a few YouTube/Netflix documentaries on how the industrial revolution via steam engines, the computer revolution, the internet revolution and most recently the cellphone revolution have changed the world and affected the way people live their lives. They were contemplative perspectives. And contemplation can sometimes lead to overfitting and cause-effect relationships misunderstood. So, a forward looking book is a good refresher for me. Here are a few things I learned so far:

- The current Artificial Intelligence revolution doesn't bring intelligence. It brings a critical component of itelligence namely prediction.
- Prediction is critical in many businesses and in people's lives. It is involved consciuosly or sub-consciously in any decision making process.
- Prediction is simply the process of filling in missing information by using the information we have (data), to generate information we do not have.

#### Cheaper predictions mean more predictions
- 200 hundred years ago, when artificial light was beginning to be used, it was 400 times more expensive than it is today. It used to be used limitedly (perhaps in certain streets and rich households). But as it got cheaper, we see it being used everywhere and things rarely imagined before became commonplace (like large building that natural sunlight cannot penetrate, lit up by artificial light). 
- The computer revolution made arithmetic cheaper. Cheap arithmetic is now being used in applications not traditionally arithmetic in nature, like photography (remember the times of dark rooms and chemicals generating our photographs?).
- Recently predictions were used for traditionally prediction intensive tasks like inventory management and demand forecasting. Off late, they are being used to manage and search our digital photos (until recently, it used to be an arithmetic only domain). And in self driving cars!
- Cheap can change strategy: today Amazon uses the shop-then-ship strategy. If predictions become extremely good (in a hypothetical future), it might be cheaper for Amazon to use the ship-then-shop model: Amazon is so sure about individual customers' future consumption pattern, that it ships them the product first and consumers have the option to return it if they don't need it. This new business model can open up many new industries (like logistics for product returns) and can transform the business.
- When something becomes cheap, its complements become more valuable. For predictions they are things like data, judgement, action and realistic simulators.

#### Prediction: What is happening today?
- With the advent of Machine Learning (deep learning in particular for many applications), the cost of producing the same quality of prediction has dropped significantly. We can see this in Computer Vision, Natural Language Processing and Speech Processing (for example). Seeing a computer vision model correctly classify tumors might seem magical. But they are just better predictions.
- In the 1990s, credit card companies caught abuot 80% of fraudulent transactions. In 2000s it was improved to 90-95% and is at 98 - 99.9% today. The last jump has been a result of machine learning. The jump looks small, but are extremely significant. An improvement from 85% to 90% means that mistakes reduce by a third. A jump from 98% to 99.9% means that mistakes fall by a factor of 20. This improvement means that credit card companies can detect and address fraud before a customer notices anything amiss. This is huge and transformational for both the company and the customer.

#### Intelligence and predictions
- Machine learning is different from statistics. Statictics cares about being right on an average. Machine Learning's goal is operational effictiveness (it can be slightly wrong all the time, yet be very useful in practice). Such an approach drove rapid improvements in the field by giving researchers the freedom to experiment. Machine learning uses many elements of statistics to be effective.
-  Machine learning can handle more complexity: consider the problem of predicting customer churn (when the customer ceases the relationship with the product/service) in the cellphone network industry. The data consists of hour by hour call records and standard variables like bill size and payment punctuality. Decades ago, a lot of human effort was involved in identifying the right kind of variables and their interactions in building a regression model. Regression, though sample efficient (it is not data hungry) limits the kind of interactions the input variables can have in the model.  While regression works well in many scenarios, machine learning has been used increasingly to predict churn. With machine learning (deep learning in particular), variables can combine in unexpected ways. For example, it can model cases like 'people with large weekend lon-distance bills who also pay late and tend to text a lot may be particularly likely to churn'. Such interactions are unthinkable in the classical world of regression. 
- Why do people refer to machine learning as artificial intelligence? The technological threshold for what an artificial intelligence is changes every few years. Today, it refers to algorithms that can perform tasks that people thought were associated with human intelligence, like object recognition and translation. However, these very same systems cannot reason and we still find it hard to interpret predictions from such complex algorithms.

#### Role of data in predictions
- For the purposes of predictions, data comes in three flavors: the input data, the training data and the feedback data
- The training data: is used to produce a good mathematical model (like obtaining the exact set of weights in a deep neural network).
- Input data: is fed to the algorithm to produce the prediction.
- Feedback data: is used to improve the model in the future.
- Data is fundamental for prediction (the right kind of data). Consider predicting the probability of stroke: this requires individual personal data. We cannot (yet) do without input data.
- Data acquisition is costly. So it is important to have a strategy for it: how frequently do you need to collect data? At what scale? More can mean costly, but can also mean better predictions. What should be the trade-off?
- If the prediction task is well defined, there are good data science tools to asses the amount of data required, given the expected reliability and the need for accuracy (as a rule of thumb: for each feature and for each value that feature can take, have atleast 5 data points for 'good' predictive capability). 
- Predicting stroke requires high frequency of data collection. So startups like Cardiogram use the Apple Watch for data collection. Contrast this with traditional ways of monitoring heart health (by visiting the doctor onces in several weeks). Collecting this data is costly - patients had to wear a device at all times which intruded with their regular routines. Since it is personal data, more engineering had to be put in place to ensure privacy.
- The economics of data collection: From a pure statistical point of view, data has decreasing returns (in terms of prediction accuracy) to scale. Information from the 10th observation is usually more valuable than information from the 1000th observation. From the economics point of view, this might not be true. Data can have increasing returns (in terms of value) to scale. Consider for example the online search industry. Users keep coming back to google since it is great on long-tail search queries (those rare ones that fail on Bing or Yahoo!). The value of a search engine is driven by its ability to give better results on unusual queries. Being a little better in search can lead to big market differences. i.e. data may have increasing returns in scale in some cases.
- Therefore organizations need to understand the relationship between adding more data, enhancing prediction accuracy, and increasing value creation.

#### On Dividing Labor with prediction machines
- Adam Smith's eighteenth century economic philosophy involved allocating labor based on relative strengths. Humans and machines both have their strengths and weaknesses. It is essential to identify these strengths and weaknesses in order to strategise on labor allocation.
- Humans are poor at prediction and are biased. There are experiments which suggest that humans are not super amazing at statistics and that they fail to account for base probability rates when making decisions. Humans also fail to account for extremeties that arise from small samples.
- Amos Tversky and researchers from Harvard Medical School presented physicians with two treatments for lung cancer, namely radiation or surgery. The 5 year survival rate recommends surgery, which is riskier than radiation. When told that "the one-month survival rate is 90%", 84% of physicians chose surgery, but the rate fell to 50% when told that "there is a 10% mortality in the first month". While both these statements mean the same thing, the framing of the sentance affected their decision. Such biases would not exist in machines.
- Similar (and sillier) biases exist in many decisions today in fields ranging from sports to policy and law.
- Machines are also poor at predictions in certain scenarios.
- Here is a very useful structure for understanding conditions under which machine predictions can fail:
- known-knowns is a scenario where we know that we have the correct and rich data to make good predictions (natural image recognition). Machines excel at these
- known-unknowns are scenarios where we know that we do not have enough data to make predictions (Predicting major earthquakes. We have extremely sparse data for this). Humans are pretty good when the data is sparse.
- unknown-unknowns are scenarios where no there is no past experience. This is what makes life interesting (at least for me). The 2008 economic meltdown was unexpected (to most people). It is extremely hard to predict such rare events. both humans and machines fail with these scenarios.
- unknown-knowns are scenarios where we wrongly think we are making the right predictions but with time realise we aren't (unobserved factors of variations that can change with time). Most machine learning models only model correlation between input data and outcomes. They do not model the underlying causal structure explicitly. Depression is a major problem among graduate students in academia. Does academia cause these grad students to be more depressed? Or are people entering academia likely to be depressed because of reasons that has nothing to do with academia? It is hard to decipher this problem, because we do not know the outcome if students had not chosen an academic path. This is where people perform controlled randomized trials (A/B testing). This is a dangerous scenario where we think the machine predicts well, but the model might not capture the underlying causal factors, in which case predictions fail when the causal factors change over time.




